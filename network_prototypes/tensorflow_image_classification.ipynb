{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Image Classification \n",
    "Author: Dennis Trollsfjord\n",
    "\n",
    "Followed tutorial by tensorflow found [here](https://www.tensorflow.org/tutorials/images/classification), with modifications adapted for our implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Installing required python packages. Then importing the packages needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install matplotlib numpy tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow_datasets as tfds  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "### Description\n",
    "The [rock paper scissor dataset](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors) is a dataset provided by [TensorFlow Datasets](https://www.tensorflow.org/datasets/overview) (TFDS). The dataset consist of 2520 images for training and 372 for testing. The images has the tensorflow shape of (300, 300, 3).\n",
    "\n",
    "### License \n",
    "\"_The dataset is licensed as a CC By 2.0, free for you to share and adapt for all uses, commercial or non-commercial. Please attribute and give appropriate credit to Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com), and place no additional restrictions on your users as outlined here._\" which can be found [here](https://laurencemoroney.com/datasets.html).\n",
    "\n",
    "### Usage\n",
    "In the following code section we load the dataset, including information about the dataset. After the dataset is loaded with the information both are asserted to verify that the type of the given objects is correct. \n",
    "\n",
    "Then the whole dataset is split into 3 parts, test, validate and train, where validate is 20% of the 'train' dataset. We take out the size of each dataset as well as the number of labels and print the result so that we know the dataset has been loaded correctly. The tensorflow dataset should result in the sizes (dataset_train : 2016), (dataset_validate : 504) and (dataset_test : 372), if the validate set is 20% of the total train set and testing is left untouched. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_info = tfds.load(\n",
    "    name='rock_paper_scissors', \n",
    "    split=[\"test\", \"train[0%:20%]\", \"train[20%:]\"],\n",
    "    data_dir='dataset', \n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "assert isinstance(dataset, list) \n",
    "assert isinstance(dataset_info, tfds.core.dataset_info.DatasetInfo)\n",
    "\n",
    "dataset_test, dataset_validate, dataset_train = dataset\n",
    "dataset_label = dataset_info.features['label'].names\n",
    "dataset_label_size = dataset_info.features['label'].num_classes\n",
    "dataset_train_size = dataset_info.splits['train[20%:]'].num_examples\n",
    "dataset_validate_size = dataset_info.splits['train[0%:20%]'].num_examples\n",
    "dataset_test_size = dataset_info.splits['test'].num_examples\n",
    "\n",
    "print(f\"dataset name: {dataset_info.name}\")  \n",
    "print(f\"dataset size: {dataset_info.dataset_size}\")\n",
    "print(f\"dataset labels: {dataset_label}\")\n",
    "print(f\"number of labels: {dataset_label_size}\")\n",
    "print(f\"train dataset size: {dataset_train_size}\") \n",
    "print(f\"validate dataset size: {dataset_validate_size}\") \n",
    "print(f\"test dataset size: {dataset_test_size}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "In the following code section, hyperparameters are defined. \n",
    "- **image_size**: is the lengths of the image in both height and width (this requires the images to be square).\n",
    "- **epochs**: is the number of epochs for training. \n",
    "- **batch_size**: is the number of samples in each mini batch. Generally a larger _batch_size_ allows for better utilization of GPU, where 32 & 64 are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "filepath = \"./saved_models/tensorflow_image_classification/\"+time+\"/\"\n",
    "\n",
    "image_size = 128\n",
    "epochs = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Formatting\n",
    "### Image Formatting\n",
    "In the following code section each of the images within the datasets are reformated. The image is resized according to the hyperparameters. The resizing is done by map over the dataset where the function which resizes is applied to each index within the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (image_size, image_size))\n",
    "    return image, label\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.map(format_image)\n",
    "dataset_validate = dataset_validate.map(format_image)\n",
    "dataset_test = dataset_test.map(format_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Buffered Prefetching\n",
    "This step is important especially for performance but also for how the dataset will function when training. \n",
    "- **cache**: Keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "- **shuffle**: Will allocate a buffer with random entries, the buffer size will have the size equal to the _buffer_size_. \n",
    "- **batch**: Will take _batch_size_ number of elements into separate batches. \n",
    "- **repeat**: Normally images are put from the dataset into the buffer and when the dataset is empty it will return an error. Repeat will re-initialize the dataset by making it again so that this error never occurs. \n",
    "- **prefetch**: Overlaps data preprocessing and model execution while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dataset_train = dataset_train.cache().shuffle(1000).batch(batch_size=batch_size).repeat().prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_validate = dataset_validate.cache().repeat().batch(batch_size=batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_test = dataset_test.cache().repeat().batch(batch_size=batch_size).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Augmentation Model\n",
    "In the following step we create a augmentation model, to avoid overfitting and allow our network to function better against new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_model = tf.keras.Sequential(\n",
    "    layers=[\n",
    "        tf.keras.layers.RandomFlip(\n",
    "            mode=\"horizontal\",\n",
    "            input_shape=(image_size, image_size, 3),\n",
    "        ),\n",
    "        tf.keras.layers.RandomRotation(factor=0.1),\n",
    "        tf.keras.layers.RandomZoom(height_factor=0.1, width_factor=0.1),\n",
    "        tf.keras.layers.GaussianNoise(stddev=0.1)\n",
    "    ], \n",
    "    name=\"augmentation_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Model\n",
    "In the following code cell we define the network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    layers=[\n",
    "        tf.keras.layers.Rescaling(\n",
    "            scale=1./255, input_shape=(image_size, image_size, 3)),\n",
    "        augmentation_model,\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=3, \n",
    "            input_shape=(image_size, image_size, 3), \n",
    "            padding='same', \n",
    "            activation='relu'\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=3, \n",
    "            input_shape=(image_size, image_size, 3), \n",
    "            padding='same', \n",
    "            activation='relu'\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            input_shape=(image_size, image_size, 3), \n",
    "            padding='same', \n",
    "            activation='relu'\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(dataset_label_size, name=\"outputs\")], \n",
    "    name=\"classification_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation\n",
    "Configures the model for training. \n",
    "- **optimizer**: The name of the optimizing method used. \n",
    "- **loss**: Loss function used. \n",
    "- **metrics**: List of metrics used to evaluate the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Of Model\n",
    "In the following section the dataset is trained. \n",
    "- **x**: The training dataset. \n",
    "- **epochs**: Number of epochs in the training. \n",
    "- **verbose**: Setting for console output. \n",
    "- **validation_data**: The validation dataset. \n",
    "- **steps_per_epoch**: Number of elements handled in each step during a single epoch.\n",
    "- **validation_steps**: Same as _steps_per_epoch_ but for the validation of a epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = model.fit(\n",
    "    x=dataset_train,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=dataset_validate,\n",
    "    steps_per_epoch=math.ceil(dataset_train_size / batch_size),\n",
    "    validation_steps=math.ceil(dataset_validate_size / batch_size),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_accuracy')],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Of Model\n",
    "In the following code section the model is evaluated by running the test dataset through the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_result = model.evaluate(\n",
    "    x=dataset_test,\n",
    "    verbose=1,\n",
    "    steps=math.ceil(dataset_test_size / batch_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = training_result.history['accuracy']\n",
    "val_acc = training_result.history['val_accuracy']\n",
    "test_acc = testing_result[1]\n",
    "\n",
    "loss = training_result.history['loss']\n",
    "val_loss = training_result.history['val_loss']\n",
    "test_loss = testing_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/keras/save_and_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(filepath+\"model\")\n",
    "\n",
    "f = open(filepath+\"result\", \"w\")\n",
    "f.write(f\"train accuracy: {acc[-1]} \\ntrain loss: {loss[-1]} \\nvalidate accuracy: {val_acc[-1]} \\nvalidate loss: {val_loss[-1]} \\ntest accuracy: {test_acc} \\ntest loss: {test_loss}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Result\n",
    "Visualization in graphs of the training, validation and testing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.plot(epochs_range, [test_acc]*len(epochs_range), label='Testing Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training, Validation and Testing Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.plot(epochs_range, [test_loss]*len(epochs_range), label='Testing Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training, Validation and Testing Loss')\n",
    "plt.savefig(filepath+\"result.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
